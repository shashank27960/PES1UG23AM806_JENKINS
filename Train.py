# -*- coding: utf-8 -*-
"""PES1UG23AM806__3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ubUUaLjz0LHRlVczIOXMNsw0fGHFMHDu
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, BatchNormalization
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# Load the MNIST dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train = x_train.reshape((x_train.shape[0], 28 * 28)).astype('float32') / 255
x_test = x_test.reshape((x_test.shape[0], 28 * 28)).astype('float32') / 255
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

def build_model(batch_norm=False):
    model = Sequential()
    model.add(Dense(100, activation='tanh', input_shape=(784,), kernel_initializer='glorot_uniform'))
    if batch_norm:
        model.add(BatchNormalization())
    model.add(Dense(100, activation='tanh', kernel_initializer='glorot_uniform'))
    if batch_norm:
        model.add(BatchNormalization())
    model.add(Dense(10, activation='softmax'))
    return model

def plot_loss(history, title):
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(title)
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

batch_sizes = [64, 128, 256, 512, 1024]
for batch_size in batch_sizes:
    model = build_model(batch_norm=False)
    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
    history = model.fit(x_train, y_train, epochs=5, batch_size=batch_size, validation_split=0.2)
    plot_loss(history, f'Batch Size {batch_size} without Batch Normalization')

    model = build_model(batch_norm=True)
    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
    history = model.fit(x_train, y_train, epochs=5, batch_size=batch_size, validation_split=0.2)
    plot_loss(history, f'Batch Size {batch_size} with Batch Normalization')

# Uniform weight initialization
model = build_model(batch_norm=False)
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(x_train, y_train, epochs=5, batch_size=128, validation_split=0.2)
plot_loss(history, 'Batch Size 128 without Batch Normalization (Uniform Init)')

model = build_model(batch_norm=True)
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(x_train, y_train, epochs=5, batch_size=128, validation_split=0.2)
plot_loss(history, 'Batch Size 128 with Batch Normalization (Uniform Init)')